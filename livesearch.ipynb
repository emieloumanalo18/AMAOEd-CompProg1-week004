{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892a094d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\emielou\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from selenium) (2022.9.24)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4218c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c91230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class liveScraping():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.Name = []\n",
    "        self.Image = []\n",
    "        self.Price = []\n",
    "        self.Link = []\n",
    "\n",
    "        \n",
    "        self.item_n = []\n",
    "        self.item_img = []\n",
    "        self.item_price = []\n",
    "        self.item_link = []\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "    def access_webpage(self, url):\n",
    "        driver = webdriver.Chrome('./chromedriver') \n",
    "        driver.get(url)\n",
    "        html = driver.page_source \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    \n",
    "                  \n",
    "    def ebay(self, url, item_max): \n",
    "        soup = self.access_webpage(url)\n",
    "        all_divs = soup.find('ul', {'class' : \"srp-results srp-grid clearfix\"})\n",
    "        \n",
    "        i = 0\n",
    "        while i != item_max:\n",
    "            for div in all_divs:\n",
    "                lis = all_divs.find_all('li')\n",
    "\n",
    "                i += 1\n",
    "                if i == item_max:\n",
    "                    break\n",
    "                    \n",
    "            for li in lis:\n",
    "                self.item_n.extend(li.find_all('div', class_='s-item__title s-item__title--with-icon'))\n",
    "                self.item_img.extend(li.findAll('img'))\n",
    "                self.item_price.extend(li.findAll('span', class_ =\"s-item__price\"))\n",
    "                self.item_link.extend(li.findAll('a' , class_ =\"s-item__link\"))\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    def amazon(self, url, item_max):\n",
    "        soup = self.access_webpage(url)\n",
    "        all_divs = soup.findAll('div', class_ = 'a-section a-spacing-base')\n",
    "        \n",
    "        i = 0\n",
    "        while i != item_max:\n",
    "            for div in all_divs:\n",
    "                self.item_n.extend(div.findAll('span', class_ = 'a-size-base-plus a-color-base a-text-normal'))\n",
    "                self.item_link.extend(div.findAll('a', class_ = 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'))\n",
    "                self.item_img.extend(div.findAll('img', class_ = 's-image'))\n",
    "                self.item_price.extend(div.findAll('span', class_ = \"a-offscreen\")) \n",
    "                \n",
    "                i += 1\n",
    "                if i == item_max:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "               \n",
    "              \n",
    " \n",
    "        \n",
    "    def flipkart(self, url, item_max):\n",
    "        soup = self.access_webpage(url)\n",
    "        all_divs = soup.findAll('div', class_ =\"_1xHGtK _373qXS\")\n",
    "        \n",
    "        i = 0\n",
    "        while i != item_max:\n",
    "            for div in all_divs:\n",
    "                self.item_n.extend(div.findAll('a', class_ = \"IRpwTa\"))\n",
    "                self.item_img.extend(div.findAll('img', class_ = \"_2r_T1I\"))\n",
    "                self.item_price.extend(div.findAll('div', class_ =\"_30jeq3\"))\n",
    "                self.item_link.extend(div.findAll('a', class_ = \"_2UzuFa\"))\n",
    "                \n",
    "                i += 1\n",
    "                if i == item_max:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "        \n",
    "    def dictionary(self, item_max, s, category, c):\n",
    "        Data = []\n",
    "            \n",
    "        for i in range(item_max):\n",
    "            data = {'Name': self.Name[i], \n",
    "                    'Image': self.Image[i],\n",
    "                    'Price': self.Price[i],\n",
    "                    'Link': self.Link[i],\n",
    "                    'Store': s,\n",
    "                    'Category': category,\n",
    "                    'Gender' : c}\n",
    "            \n",
    "            Data.append(data)\n",
    "            \n",
    "        return Data\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def  Scrape(self, s, url, c, item_max, g):\n",
    "        store = s.lower() \n",
    "        time.sleep(5) \n",
    "        \n",
    "                    \n",
    "        if store == \"ebay\":\n",
    "            self.ebay(url, item_max)\n",
    "        elif store == \"amazon\":\n",
    "            self.amazon(url, item_max)\n",
    "        elif store == \"flipkart\":\n",
    "            self.flipkart(url, item_max)\n",
    "        elif store == \"shopee\":\n",
    "            self.shopee(url, item_max)\n",
    "        else: print('Sorry, the store is not available for scraping.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        for name in self.item_n:\n",
    "            self.Name.append(name.get_text())\n",
    "\n",
    "        for img in self.item_img:\n",
    "            self.Image.append(img['src'])\n",
    "\n",
    "        for price in self.item_price:\n",
    "            self.Price.append(price.get_text())\n",
    "\n",
    "        for link in self.item_link:\n",
    "            self.Link.append(link['href'])\n",
    "            \n",
    "        Data = self.dictionary(item_max, s, c, g)\n",
    "        \n",
    "        return Data\n",
    "    \n",
    "      \n",
    "       \n",
    "    \n",
    "        \n",
    "#  compiling diff stores   \n",
    "    def CompileDataset(self, d):\n",
    "        dataset = []\n",
    "        d_len = len(d)\n",
    "\n",
    "        for i in range(d_len):\n",
    "            for c in range(len(d_c[c])):\n",
    "                dataset.append(d[i][c])\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# compiling diff category\n",
    "    def buildDataset(self, param):\n",
    "        dataset = []\n",
    "\n",
    "        for i in range(len(param)):\n",
    "            webpage_url = param[i]['url']\n",
    "            store = param[i]['Store']\n",
    "            max_item = param[i]['item_max']\n",
    "            gender = param[i]['Gender']\n",
    "            category = param[i]['Category']\n",
    "            \n",
    "            dataset.extend(self.Scrape(store, webpage_url, category, max_item, gender))\n",
    "            \n",
    "            print(f'{category} has been added.')\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9051522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = liveScraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745b8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[{\n",
    "    \"url\" :'https://www.ebay.ph/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=shirts+&_sacat=0',\n",
    "    \"Store\":'eBay',\n",
    "    \"Category\": 'Shirts',\n",
    "    \"item_max\": 20,\n",
    "    \"Gender\": \"Woman\"\n",
    "    },{\n",
    "     \"url\" :'https://www.amazon.com/s?k=dress&crid=2MW0I0KDNE4LR&sprefix=dres%2Caps%2C851&ref=nb_sb_noss_2',\n",
    "    \"Store\":'Amazon',\n",
    "    \"item_max\": 20,\n",
    "    \"Category\": 'Dress',\n",
    "    \"Gender\": \"Woman\"\n",
    "    },{\n",
    "    \"url\" :'https://www.flipkart.com/search?q=dress&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off',\n",
    "    \"Store\":'Flipkart',\n",
    "    \"item_max\": 20,\n",
    "    \"Category\": 'Dress',\n",
    "    \"Gender\": \"Woman\"\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e7f94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emielou\\AppData\\Local\\Temp\\ipykernel_12240\\2464320037.py:19: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shirts has been added.\n",
      "Dress has been added.\n",
      "Dress has been added.\n"
     ]
    }
   ],
   "source": [
    "db = sp.buildDataset(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0433042",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('C:/Users/emielou/Desktop/scraping/dataset.json', 'w', encoding='latin-1') as f:\n",
    "    json.dump(db, f, indent=8, ensure_ascii=False)\n",
    "    print('Done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
