{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee7f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emielou\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78bdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# live scraping\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import json\n",
    "\n",
    "#  classification\n",
    "from fastai.vision.all import *\n",
    "\n",
    "#  recommendation\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9af267f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Recommendation():\n",
    "    def __init__(self):\n",
    "        self.n = settings['n']\n",
    "        self.eta = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "\n",
    "    \n",
    "    \n",
    "    def access_webpage(self, url):\n",
    "        driver = webdriver.Chrome('./chromedriver') \n",
    "        driver.get(url)\n",
    "        html = driver.page_source \n",
    "#         print(html)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    \n",
    "    def scrape_data(self, container, items, item_len):\n",
    "        item_n = [] \n",
    "        item_link = []\n",
    "        item_img = []\n",
    "        item_price = []\n",
    "        \n",
    "        i = 0\n",
    "        while i != item_len:\n",
    "            for div in container:\n",
    "                for item in items:\n",
    "                    \n",
    "                    img_len = len(items[2])\n",
    "                    item_name = item[0].lower()\n",
    "                    \n",
    "                    data_fetched = div.findAll(item[1], class_ = item[2])\n",
    "\n",
    "                    if item_name == 'name':\n",
    "                        item_n.extend(data_fetched)\n",
    "                    elif item_name == 'link':\n",
    "                        item_link.extend(data_fetched)\n",
    "                    elif item_name == 'image':\n",
    "                        item_img.extend(data_fetched)\n",
    "                    elif item_name == 'price':\n",
    "                        item_price.extend(data_fetched)\n",
    "                    \n",
    "                    \n",
    "                i += 1\n",
    "                if i == item_len:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "        return  item_n, item_link, item_img, item_price\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Filter(self, item_n, item_link, item_img, item_price):\n",
    "        names = []\n",
    "        prices = []\n",
    "        links = []\n",
    "        imgs = []\n",
    "        \n",
    "        for name in item_n:\n",
    "            names.append(name.get_text())\n",
    "            \n",
    "        for link in item_link:\n",
    "            links.append(link['href'])\n",
    "            \n",
    "        for price in item_price:\n",
    "            prices.append(price.get_text())\n",
    "            \n",
    "        for img in item_img:\n",
    "            imgs.append(img['src'])\n",
    "        \n",
    "        \n",
    "        return names, prices, links, imgs\n",
    "            \n",
    "            \n",
    "        \n",
    "#  fetch items    \n",
    "    def scrape(self, directories):\n",
    "        self.dataset = []\n",
    "        \n",
    "        for c in range(len(directories)):\n",
    "            f = open(directories[c])\n",
    "            parameters = json.load(f)\n",
    "            \n",
    "            \n",
    "            for param in parameters:\n",
    "                time.sleep(5)\n",
    "                \n",
    "                soup = self.access_webpage(param['url'])\n",
    "                \n",
    "                con = param['main_container']\n",
    "                container = soup.findAll(con[0], class_ = con[1])\n",
    "#                 print(container)\n",
    "                item_n, item_link, item_img, item_price = self.scrape_data(container, param['fetch_data_item'], param['item_len'])\n",
    "                \n",
    "                names, prices, links, imgs = self.Filter(item_n, item_link, item_img, item_price)\n",
    "\n",
    "\n",
    "                for i in range(param['item_len']):\n",
    "                    data = {\n",
    "                        'Name': names[i],\n",
    "                        'Price': prices[i],\n",
    "                        'Image': imgs[i],\n",
    "                        'Link' : links[i],\n",
    "                        'Category': param['Category'],\n",
    "                        'Store' : param['Store'],\n",
    "                        'Gender' : param['Gender']\n",
    "                    }\n",
    "\n",
    "                    self.dataset.append(data)\n",
    "\n",
    "                print(f'Store: {param[\"Store\"]} Item: {param[\"Category\"]} Gender: {param[\"Store\"]} ')\n",
    "            \n",
    "        return self.dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def CM_Models(self, gender, img):\n",
    "        cm_path = \"C:\\\\Users\\\\emielou\\\\Desktop\\\\scraping\\\\ClassificationModels\\\\\"\n",
    "\n",
    "        if gender.lower() == 'man':\n",
    "            cmodel = load_learner(f'{cm_path}m_classification\\\\m_classification.pkl')\n",
    "        else:\n",
    "            cmodel = load_learner(f'{cm_path}w_classification\\\\w_classification.pkl')\n",
    "\n",
    "        img_category = cmodel.predict(item = img)\n",
    "\n",
    "        return img_category[0], gender\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def display_classified(self, img_category, gender):\n",
    "        Items = []\n",
    "        \n",
    "        for data in self.dataset:\n",
    "            Items.append(data)\n",
    "        \n",
    "        display_items = []\n",
    "        for i in range(len(Items)):\n",
    "            if Items[i]['Category'].lower() == img_category.lower() and Items[i]['Gender'].lower() == gender.lower():\n",
    "                display_items.append(Items[i])\n",
    "        \n",
    "        \n",
    "        return display_items\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_training_data(self, dataset):\n",
    "        data_pairs = []\n",
    "\n",
    "        for item in dataset:\n",
    "            X = item['Name']\n",
    "            Y = [item['Price'], item['Gender'], item['Category']]\n",
    "            data_pairs.append([X,Y])\n",
    "\n",
    "\n",
    "        count = defaultdict(int)\n",
    "        c_item = defaultdict(int)\n",
    "        \n",
    "        for data in data_pairs:\n",
    "            count[data[0]] + 1\n",
    "            c_item[data[0]] + 1\n",
    "\n",
    "            for yc in data[1]:\n",
    "                count[yc] +1\n",
    "\n",
    "        self.len_d = len(count.keys())\n",
    "        self.len_item = len(c_item.keys())\n",
    "        \n",
    "        self.list_x = sorted(list(count.keys()), reverse=False)\n",
    "        self.x_index = {x:i for (i, x) in enumerate(self.list_x)}  \n",
    "        self.index_x = {i:x for (i, x) in enumerate(self.list_x)}\n",
    "        \n",
    "        self.item_list = sorted(list(c_item.keys()), reverse=False)\n",
    "        self.x_item = {x:i for (i, x) in enumerate(self.item_list)}  \n",
    "        self.item_x = {i:x for (i, x) in enumerate(self.item_list)}\n",
    "\n",
    "\n",
    "        training_data = []\n",
    "        for data in data_pairs:\n",
    "            target = self.x_index[data[0]]\n",
    "            i_target = [0 for i in range(0, self.len_d )]\n",
    "\n",
    "            i_target[target] = 1\n",
    "\n",
    "            i_content = []\n",
    "            for y_data in data[1]:\n",
    "                content = self.x_index[y_data]\n",
    "                item_content = [0 for i in range(0, self.len_d )]\n",
    "                item_content[content] = 1\n",
    "\n",
    "                i_content.append(item_content)\n",
    "\n",
    "            training_data.append([i_target, i_content])\n",
    "\n",
    "\n",
    "        return np.array(training_data, dtype=object)\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    def softmax(self, X):\n",
    "        e_x = np.exp(X - np.max(X))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        h = np.dot(self.w1.T, x)\n",
    "        u = np.dot(self.w2.T, h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "                \n",
    "\n",
    "        \n",
    "        \n",
    "    def backprop(self, e, h, x): \n",
    "        dl_dw2 = np.outer(h, e)  \n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        self.w1 = self.w1 - (self.eta * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * dl_dw2)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, dataset):\n",
    "        training_data = self.generate_training_data(dataset)\n",
    "        \n",
    "        self.w1 = np.random.uniform(-1, 1, (self.len_d, self.n) )\n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.len_d) )\n",
    "             \n",
    "        self.loss = 0\n",
    "\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            for data in training_data:\n",
    "                y_pred, h, u = self.forward_pass(data[0])\n",
    "                \n",
    "                EI = np.sum([(y_pred - y) for y in data[1]], axis=0)\n",
    "                self.backprop(EI, h, data[0])\n",
    "\n",
    "                self.loss =  -np.sum([u[y.index(1)] for y in data[1]]) + len(data[1]) * np.log(np.sum(np.exp(u)))\n",
    "                 \n",
    "            print('Iteration: ',i, ' Loss: ', self.loss)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def word_vec(self, item):\n",
    "        i_index = self.x_index[item]\n",
    "        item_vec = self.w1[i_index]\n",
    "        \n",
    "        return item_vec\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def recommendation(self,dataset, item, item_len):\n",
    "        item_name = []\n",
    "        item_embed = []\n",
    "        item_sim = {}\n",
    "        \n",
    "        v_w1 =  self.word_vec(item)\n",
    "        \n",
    "        for data in dataset:\n",
    "            item_name.append(data['Name'])\n",
    "            item_embed.append(self.word_vec(data['Name']))\n",
    "            \n",
    "        \n",
    "        for i in range(len(item_name)):\n",
    "            v_w2 = item_embed[i]\n",
    "            A = np.dot(v_w1, v_w2)\n",
    "            B = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = A / B\n",
    "            \n",
    "            word = item_name[i]\n",
    "            item_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(item_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "        get_word = []\n",
    "        get_sim = []\n",
    "        \n",
    "        for word, sim in words_sorted[:item_len]:\n",
    "            get_word.append(word)\n",
    "            get_sim.append(sim)\n",
    "            \n",
    "        return (get_word, get_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef68c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'n': 100,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dfb8aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "srp = Classification_Recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d482a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = ['C:/Users/emielou/Desktop/scraping/Queries/flipkart_queries.json', 'C:/Users/emielou/Desktop/scraping/Queries/amazon_queries.json'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cd558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emielou\\AppData\\Local\\Temp\\ipykernel_29772\\3571986583.py:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store: Flipkart Item: Jacket Gender: Flipkart \n",
      "Store: Flipkart Item: Jeans Gender: Flipkart \n",
      "Store: Flipkart Item: Pants Gender: Flipkart \n",
      "Store: Flipkart Item: Shorts Gender: Flipkart \n",
      "Store: Flipkart Item: Shirts Gender: Flipkart \n",
      "Store: Flipkart Item: Belt Gender: Flipkart \n",
      "Store: Flipkart Item: Dress Gender: Flipkart \n",
      "Store: Flipkart Item: Jeans Gender: Flipkart \n",
      "Store: Flipkart Item: Pants Gender: Flipkart \n",
      "Store: Flipkart Item: Shirts Gender: Flipkart \n",
      "Store: Flipkart Item: Belt Gender: Flipkart \n",
      "Store: Flipkart Item: Skirts Gender: Flipkart \n",
      "Store: Flipkart Item: Shorts Gender: Flipkart \n",
      "Store: Amazon Item: Jacket Gender: Amazon \n",
      "Store: Amazon Item: Jeans Gender: Amazon \n",
      "Store: Amazon Item: Pants Gender: Amazon \n",
      "Store: Amazon Item: Shorts Gender: Amazon \n",
      "Store: Amazon Item: Shirts Gender: Amazon \n",
      "Store: Amazon Item: Belt Gender: Amazon \n",
      "Store: Amazon Item: Dress Gender: Amazon \n",
      "Store: Amazon Item: Jeans Gender: Amazon \n",
      "Store: Amazon Item: Pants Gender: Amazon \n",
      "Store: Amazon Item: Shirts Gender: Amazon \n",
      "Store: Amazon Item: Shorts Gender: Amazon \n",
      "Store: Amazon Item: Belt Gender: Amazon \n",
      "Store: Amazon Item: SkirtS Gender: Amazon \n"
     ]
    }
   ],
   "source": [
    "dataset = srp.scrape(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9449de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Loss:  34.07533238523474\n",
      "Iteration:  1  Loss:  30.544213903632397\n",
      "Iteration:  2  Loss:  27.69982348933522\n",
      "Iteration:  3  Loss:  25.6179259620135\n",
      "Iteration:  4  Loss:  24.0132869873837\n",
      "Iteration:  5  Loss:  22.55082333268663\n",
      "Iteration:  6  Loss:  21.13400959750671\n",
      "Iteration:  7  Loss:  19.746824120263163\n",
      "Iteration:  8  Loss:  18.37932383865548\n",
      "Iteration:  9  Loss:  17.028755012539904\n"
     ]
    }
   ],
   "source": [
    "srp.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1a52daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name = []\n",
    "for data in dataset:\n",
    "    item_name.append(data['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6b16248f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women Regular Fit Green Cotton Blend Trousers'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_name[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "54cbd031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_vec = srp.word_vec(item_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "37a021ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = srp.recommendation(dataset, item_name[0], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc096b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
