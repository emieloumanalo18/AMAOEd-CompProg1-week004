{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c603b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import functools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92e01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold the weights of all layer in NN  256x256 = 65,536 inputs\n",
    "def layers_weights(last_layer, initial=True):\n",
    "    network_weights = []\n",
    "    layer = last_layer\n",
    "    \n",
    "    while \"previous_layer\" in layer.__init__.__code__.co_varnames:\n",
    "        if initial == True:\n",
    "            network_weights.append(layer.initial_weights)\n",
    "        elif initial == False:\n",
    "            network_weights.append(layer.trained_weights)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected value to the 'initial' parameter: {initial}.\".format(initial=initial))\n",
    "        \n",
    "        layer = layer.previous_layer\n",
    "\n",
    "    if not (type(layer) is InputLayer):\n",
    "         raise TypeError(\"The first layer in the network architecture must be an input layer.\")\n",
    "    \n",
    "    network_weights.reverse()\n",
    "    return numpy.array(network_weights)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the layers_weights to vector \n",
    "def layers_weights_as_vector(last_layer, initial = True):\n",
    "    network_weights = []\n",
    "    layer = last_layer\n",
    "    \n",
    "    while \"previous_layer\" in layer.__init__.__code__.co_varnames:\n",
    "        if initial == True:\n",
    "            vector = numpy.reshape(layer.initial_weights, newshape=(layer.initial_weights.size))\n",
    "            network_weights.extend(vector)\n",
    "        elif initial == False:\n",
    "            vector = numpy.reshape(layer.trained_weights, newshape=(layer.trained_weights.size))\n",
    "            network_weights.extend(vector)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected value to the 'initial' parameter: {initial}.\".format(initial=initial))\n",
    "\n",
    "        layer = layer.previous_layer\n",
    "        \n",
    "    if not (type(layer) is InputLayer):\n",
    "        raise TypeError(\"The first layer in the network architecture must be an input layer.\")\n",
    "    \n",
    "    network_weights.reverse()\n",
    "    return numpy.array(network_weights)\n",
    "\n",
    "\n",
    "\n",
    "#convert vector weights to matrix\n",
    "def layers_weights_as_matrix(last_layer, vector_weights):\n",
    "    network_weight = []\n",
    "    \n",
    "    start = 0\n",
    "    layer = last_layer\n",
    "    vector_weights= vector_weights[::-1]\n",
    "    \n",
    "    while \"previous_layer\" in layer.__init__.__code__.co_varnames:\n",
    "        layer_weights_shape = layer.initial_weights.shape\n",
    "        layer_weights_size = layer.initial_weights.shape\n",
    "        \n",
    "        vector_weight = vector_weights[start:start + layer_weights_size]\n",
    "        matrix = numpy.reshape(vector_weight, newshape=(layer_weights_shape))\n",
    "        network_weights.append(matrix)\n",
    "        \n",
    "        start = start + layer_weights_size\n",
    "        \n",
    "        layer = layer.previous_layer\n",
    "    \n",
    "    if not (type(layer) is InputLayer):\n",
    "        raise TypeError(\"The first layer in the network architecture must be an input layer.\")\n",
    "    \n",
    "    network_weights.reverse()\n",
    "    return network_weights\n",
    "        \n",
    "    \n",
    "\n",
    "def layers_activations(last_layer):\n",
    "    activations = []\n",
    "    layer = last_layer\n",
    "    \n",
    "    while \"previous_layer\" in layer.__init__.__code__.co_varnames:\n",
    "        activations.append(layer.activation_function)\n",
    "        \n",
    "        layer = layer.previous_layer\n",
    "        \n",
    "    if not (type(layer) is InputLayer):\n",
    "        raise TypeError(\"The first layer in the network architecture must be an input layer.\")\n",
    "    \n",
    "    network_weights.reverse()\n",
    "    return network_weights\n",
    "        \n",
    "\n",
    "    \n",
    "def sigmoid(input):\n",
    "    if type(input) in [list, tuple]:\n",
    "        input = numpy.array(input)\n",
    "        \n",
    "    return 1.0/(1 + numpy.exp(-1 * input))\n",
    "\n",
    "\n",
    "\n",
    "# apply rectified linear unit (relu) function\n",
    "def relu(input):\n",
    "    if not (type(input) in [list, tuple, numpy.ndarray]):\n",
    "        if input < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return input\n",
    "    elif type(input) in [list, tuple]:\n",
    "        input = numpy.array(input)\n",
    "\n",
    "    result = input\n",
    "    result[input < 0] = 0\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "    \n",
    "def softmax(layer_outputs):\n",
    "    return layer_outputs / (numpy.sum(layer_outputs) + 0.000001)\n",
    "\n",
    "\n",
    "\n",
    "def train(num_epochs, \n",
    "          last_layer, \n",
    "          data_inputs, \n",
    "          data_outputs,\n",
    "          problem_type=\"classification\",\n",
    "          learning_rate=0.01):\n",
    "    \n",
    "   # fetch the initial weights of the layer\n",
    "    weights = layers_weights(last_layer, initial=True)\n",
    "    activations = layers_activations(last_layer)\n",
    "    \n",
    "    nnNetwork_error = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# how far the target to the predicted    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        for sample_idx in range(data_inputs.shape[0]):\n",
    "            r1 = data_inputs[sample_idx, :]\n",
    "            for idx in range(len(weights) -1):\n",
    "                curr_weights = weights[idx]\n",
    "                r1 = numpy.matmul(r1, curr_weights)\n",
    "                \n",
    "                if activations[idx] == \"relu\":\n",
    "                    r1 = relu(r1)\n",
    "                elif activations[idx] == \"sigmoid\":\n",
    "                    r1 = sigmoid(r1)\n",
    "                elif activations[idx] == \"softmax\":\n",
    "                    r1 = softmax(r1)\n",
    "                elif activations[idx] == None:\n",
    "                    pass\n",
    "                \n",
    "            curr_weights = weights[-1]\n",
    "            r1 = numpy.matmul(r1, curr_weights)\n",
    "            \n",
    "            if problem_type == \"classification\":\n",
    "                prediction = numpy.where(r1 == numpy.max(r1))[0][0]\n",
    "            else:\n",
    "                prediction = r1\n",
    "            \n",
    "            network_error = network_error + numpy.mean(numpy.abs((prediction - data_outputs[sample_idx])))\n",
    "\n",
    "# update network weights after completing an epoch \n",
    "        weights = update_weights(weights=weights,\n",
    "                                 network_error=network_error,\n",
    "                                 learning_rate=learning_rate)\n",
    "    \n",
    "    update_layers_trained_weights(last_layer, weights)\n",
    "        \n",
    "\n",
    "        \n",
    "# update weight using lrate        \n",
    "def update_weights(weights, network_err, l_rate):\n",
    "    for layer_idx in range(len(weights)):\n",
    "        weights[layer_idx] = network_err * l_rate * weights[layer_idx]\n",
    "        \n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_layers_trained_weights(last_layer, final_weights):\n",
    "    layer = last_layer\n",
    "    layer_idx = len(final_weights) - 1\n",
    "    \n",
    "    while \"previous_layer\" in  layer.__init__.__code__.co_varnames:\n",
    "        layer.trained_weights = final_weights[layer_idx]\n",
    "        \n",
    "        layer_idx = layer_idx - 1\n",
    "        \n",
    "        layer = layer.previous_layer\n",
    "        \n",
    "        \n",
    "def predict(last_layer, data_inputs, problem_type=\"classification\"):\n",
    "    \n",
    "    weights = layers_weights(last_layer, initial=False)\n",
    "    activations = layers_activations(last_layer)\n",
    "\n",
    "    \n",
    "    if len(weights) != len(activations):\n",
    "        raise TypeError(\"The length of layers {num_layers} is not equal to the number of activations functions {num_activations} and they must be equal.\".format(num_layers=len(weights), num_activations=len(activations)))\n",
    "        \n",
    "    predictions = []\n",
    "    for sample_idx in range(data_inputs.shape[0]):\n",
    "        r1 = data_inputs[sample_idx, :]\n",
    "        for curr_weights, activation in zip(weights, activations):\n",
    "            r1 = numpy.matmul(r1, curr_weights)\n",
    "            if activation == \"relu\":\n",
    "                r1 = relu(r1)\n",
    "            elif activation == \"sigmoid\":\n",
    "                r1 = sigmoid(r1)\n",
    "            elif activation == \"softmax\":\n",
    "                r1 = softmax(r1)\n",
    "            elif activation == None:\n",
    "                pass\n",
    "\n",
    "        if problem_type == \"classification\":\n",
    "            prediction = numpy.where(r1 == numpy.max(r1))[0][0]\n",
    "        else:\n",
    "            prediction = r1\n",
    "\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_vector(array):\n",
    "    if not (type(array) is numpy.ndarray):\n",
    "        raise TypeError(\"An input of type numpy.ndarray is expected but an input of type {in_type} found.\".format(in_type=type(array)))\n",
    "    return numpy.reshape(array, newshape=(array.size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_array(vector, shape):\n",
    "    \n",
    "    if not (type(vector) is numpy.ndarray):\n",
    "        raise TypeError(\"An input of type numpy.ndarray is expected but an input of type {in_type} found.\".format(in_type=type(vector)))\n",
    "        \n",
    "    if vector.ndim > 1:\n",
    "        raise ValueError(\"A 1D NumPy array is expected but an array of {ndim} dimensions found.\".format(ndim=vector.ndim))\n",
    "        \n",
    "    if vector.size != functools.reduce(lambda x,y:x*y, shape, 1):\n",
    "        raise ValueError(\"Mismatch between the vector length and the array shape. A vector of length {vector_length} cannot be converted into a array of shape ({array_shape}).\".\n",
    "                         format(vector_length=vector.size, array_shape=shape))\n",
    "        \n",
    "    return numpy.reshape(vector, newshape=shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be566387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self, num_inputs):\n",
    "          if num_inputs <= 0:\n",
    "            raise ValueError(\"Number of input neurons cannot be <= 0. Please pass a valid value to the 'num_inputs' parameter.\")\n",
    "        \n",
    "          self.num_neurons = num_inputs\n",
    "            \n",
    "            \n",
    "            \n",
    "class DenseLayer:\n",
    "    def __init__(self, num_neurons, previous_layer, activation_function=\"sigmoid\"):\n",
    "        \n",
    "        if num_neurons <= 0:\n",
    "            raise ValueError(\"Number of neurons cannot be <= 0. Please pass a valid value to the 'num_neurons' parameter.\")\n",
    "            \n",
    "        # Number of neurons in the dense layer.\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "        supported_activation_functions = (\"sigmoid\", \"relu\", \"softmax\", \"None\")\n",
    "        if not (activation_function in supported_activation_functions):\n",
    "            raise ValueError(\"The specified activation function '{activation_function}' is not among the supported activation functions {supported_activation_functions}. Please use one of the supported functions.\".format(activation_function=activation_function, supported_activation_functions=supported_activation_functions))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        if previous_layer is None:\n",
    "            raise TypeError(\"The previous layer cannot be of Type 'None'. Please pass a valid layer to the 'previous_layer' parameter.\")\n",
    "       \n",
    "        self.previous_layer = previous_layer\n",
    "\n",
    "        # Initialize the weights of the layer.\n",
    "        self.initial_weights = numpy.random.uniform(low=-0.1,\n",
    "                                                    high=0.1,\n",
    "                                                    size=(previous_layer.num_neurons, num_neurons))\n",
    "        \n",
    "        \n",
    "        self.trained_weights = self.initial_weights.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
