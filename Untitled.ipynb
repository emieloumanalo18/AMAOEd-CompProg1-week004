{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97350f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emielou\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\emielou\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5ff970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8441907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class livescraping():\n",
    "    def __init__(self):\n",
    "        self.names = []\n",
    "        self.prices = []\n",
    "        self.links = []\n",
    "        self.imgs = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    def access_webpage(self, url):\n",
    "        driver = webdriver.Chrome('./chromedriver') \n",
    "        driver.get(url)\n",
    "        html = driver.page_source \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    \n",
    "    def scrape_data(self, container, items, item_len):\n",
    "        item_n = [] \n",
    "        item_link = []\n",
    "        item_img = []\n",
    "        item_price = []\n",
    "        \n",
    "        i = 0\n",
    "        while i != item_len:\n",
    "            for div in container:\n",
    "                for item in items:\n",
    "                    \n",
    "                    img_len = len(items[2])\n",
    "                    item_name = item[0].lower()\n",
    "                    \n",
    "                    data_fetched = div.findAll(item[1], class_ = item[2])\n",
    "\n",
    "                    if item_name == 'name':\n",
    "                        item_n.extend(data_fetched)\n",
    "                    elif item_name == 'link':\n",
    "                        item_link.extend(data_fetched)\n",
    "                    elif item_name == 'image':\n",
    "                        item_img.extend(data_fetched)\n",
    "                    elif item_name == 'price':\n",
    "                        item_price.extend(data_fetched)\n",
    "                    \n",
    "                    \n",
    "                i += 1\n",
    "                if i == item_len:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "        return  item_n, item_link, item_img, item_price\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    def Filter(self, item_n, item_link, item_img, item_price):\n",
    "        names = []\n",
    "        prices = []\n",
    "        links = []\n",
    "        imgs = []\n",
    "        \n",
    "        for name in item_n:\n",
    "            names.append(name.get_text())\n",
    "            \n",
    "        for link in item_link:\n",
    "            links.append(link['href'])\n",
    "            \n",
    "        for price in item_price:\n",
    "            prices.append(price.get_text())\n",
    "            \n",
    "        for img in item_img:\n",
    "            imgs.append(img['src'])\n",
    "        \n",
    "        return names, prices, links, imgs\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "    def scrape(self, directories):\n",
    "        dataset = []\n",
    "        \n",
    "        for c in range(len(directories)):\n",
    "            f = open(directories[c])\n",
    "            parameters = json.load(f)\n",
    "            \n",
    "            for param in parameters:\n",
    "                time.sleep(5)\n",
    "                \n",
    "                soup = self.access_webpage(param['url'])\n",
    "\n",
    "                con = param['main_container']\n",
    "                container = soup.findAll(con[0], class_ = con[1])\n",
    "#                 print(container)\n",
    "\n",
    "                item_n, item_link, item_img, item_price = self.scrape_data(container, param['fetch_data_item'], param['item_len'])\n",
    "                names, prices, links, imgs = self.Filter(item_n, item_link, item_img, item_price)\n",
    "\n",
    "\n",
    "                for i in range(param['item_len']):\n",
    "                    data = {\n",
    "                        'Name': names[i],\n",
    "                        'Price': prices[i],\n",
    "                        'Image': imgs[i],\n",
    "                        'Link' : links[i]\n",
    "                    }\n",
    "\n",
    "                    dataset.append(data)\n",
    "\n",
    "                print(f'all {param[\"item_len\"]}  {param[\"Category\"]} items from {param[\"Store\"]} has been scraped.')\n",
    "            \n",
    "        return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b732cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "srp = livescraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a791518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = ['C:/Users/emielou/Desktop/scraping/Queries/eBay_queries.json','C:/Users/emielou/Desktop/scraping/Queries/amazon_queries.json','C:/Users/emielou/Desktop/scraping/Queries/flipkart_queries.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d9b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emielou\\AppData\\Local\\Temp\\ipykernel_16296\\1837813306.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 10  Jacket items from eBay has been scraped.\n",
      "all 10  Jeans items from eBay has been scraped.\n",
      "all 10  Pants items from eBay has been scraped.\n",
      "all 10  Shorts items from eBay has been scraped.\n",
      "all 10  Shirts items from eBay has been scraped.\n",
      "all 10  Belt items from eBay has been scraped.\n",
      "all 10  Dress items from eBay has been scraped.\n",
      "all 10  Jeans items from eBay has been scraped.\n",
      "all 10  Pants items from eBay has been scraped.\n",
      "all 10  Shirts items from eBay has been scraped.\n",
      "all 10  Shorts items from eBay has been scraped.\n",
      "all 10  Belt items from eBay has been scraped.\n"
     ]
    }
   ],
   "source": [
    "dataset = srp.scrape(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207a4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
